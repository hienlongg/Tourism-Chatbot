"""
RAG Tourism Recommendation Engine

This module contains the core RAG (Retrieval-Augmented Generation) functionality
for Vietnamese tourism recommendations. It handles data loading, document creation,
vector store initialization, and recommendation generation using LangChain and Gemini.

Key Components:
- Data loading and processing from CSV
- Document creation with metadata
- Vector embeddings using HuggingFace
- ChromaDB vector store for semantic search
- LLM-based recommendation generation with Gemini

"""

import pandas as pd
import unicodedata
import re
import os
from typing import List, Dict, Tuple, Optional
import warnings

# LangChain imports
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate

# Application imports
from config import Config

warnings.filterwarnings('ignore')


# ============================================================================
# CONFIGURATION
# ============================================================================

# Import configuration from config.py
CSV_PATH = Config.RAG_CSV_PATH
CHROMA_DB_PATH = Config.RAG_CHROMA_DB_PATH
EMBEDDING_MODEL = Config.RAG_EMBEDDING_MODEL
GEMINI_MODEL = Config.RAG_GEMINI_MODEL
TOP_K_RESULTS = Config.RAG_TOP_K_RESULTS
LLM_TEMPERATURE = Config.RAG_LLM_TEMPERATURE
GEMINI_API_KEY = Config.GEMINI_API_KEY


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def slugify(value: str) -> str:
    """
    Convert Vietnamese text with diacritics into a URL-safe slug.
    
    Purpose: Create unique, safe identifiers for each location.
    
    Example:
        "Khu nhÃ  cÃ´ng tá»­ Báº¡c LiÃªu" -> "khu_nha_cong_tu_bac_lieu"
        "ThÃ¡c Khe Váº±n" -> "thac_khe_van"
    
    Args:
        value: Vietnamese location name with diacritics
    
    Returns:
        Lowercase slug with underscores (URL-safe)
    """
    # Step 1: Convert Vietnamese 'Ä‘' to 'd' (not handled by NFKD)
    value = str(value).replace("Ä‘", "d").replace("Ä", "D")
    
    # Step 2: Remove Vietnamese diacritics using Unicode normalization
    value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('utf-8')
    
    # Step 3: Remove special characters, keep only alphanumeric and spaces
    value = re.sub(r'[^\w\s-]', '', value).strip().lower()
    
    # Step 4: Replace spaces and hyphens with underscores
    value = re.sub(r'[\s-]+', '_', value)
    
    return value


# ============================================================================
# DATA LOADING & PROCESSING
# ============================================================================

def load_and_process_data(csv_path: str) -> pd.DataFrame:
    """
    Load tourism location CSV and prepare it for vectorization.
    
    Purpose: 
    - Load raw CSV data
    - Generate loc_id for each location
    - Filter out rows with missing critical fields
    - Set loc_id as index for easy lookup
    
    Args:
        csv_path: Path to the CSV file
    
    Returns:
        Processed DataFrame with loc_id as index
    """
    print("ðŸ“‚ Loading CSV data...")
    df = pd.read_csv(csv_path)
    print(f"   Loaded {len(df)} rows")
    
    # Generate loc_id using slugify
    print("ðŸ”‘ Generating loc_id for each location...")
    df['loc_id'] = df['TenDiaDanh'].apply(slugify)
    
    # Filter: Keep only rows with TenDiaDanh, DiaChi (NoiDung can be null)
    print("ðŸ” Filtering rows with missing critical data...")
    df_filtered = df.dropna(subset=['TenDiaDanh', 'DiaChi']).copy()
    
    # Fill NaN in NoiDung with empty string
    df_filtered['NoiDung'] = df_filtered['NoiDung'].fillna('')
    
    print(f"   Kept {len(df_filtered)} rows after filtering")
    
    # Set loc_id as index for fast lookup
    df_filtered = df_filtered.set_index('loc_id')
    
    return df_filtered


def create_documents(df: pd.DataFrame) -> List[Document]:
    """
    Convert DataFrame rows into LangChain Document objects.
    
    Purpose:
    - Create rich text content for embedding (TenDiaDanh + NoiDung + DiaChi)
    - Store metadata (loc_id, original columns) for retrieval
    - Each Document will be embedded and stored in ChromaDB
    
    Document Structure:
    - page_content: Combined text for semantic search
    - metadata: All original fields for context generation
    
    Args:
        df: Processed DataFrame with loc_id as index
    
    Returns:
        List of LangChain Document objects
    """
    print("ðŸ“ Creating documents for vectorization...")
    documents = []
    
    for loc_id, row in df.iterrows():
        # Build rich content: Title + Description + Location
        # This combined text will be embedded for semantic search
        content_parts = [
            f"TÃªn Ä‘á»‹a danh: {row['TenDiaDanh']}",
            f"Äá»‹a chá»‰: {row['DiaChi']}"
        ]
        
        # Add description if available
        if row['NoiDung'] and str(row['NoiDung']).strip():
            content_parts.append(f"MÃ´ táº£: {row['NoiDung']}")
        
        page_content = "\n".join(content_parts)
        
        # Store all metadata for later use in recommendations
        metadata = {
            'loc_id': loc_id,
            'TenDiaDanh': row['TenDiaDanh'],
            'DiaChi': row['DiaChi'],
            'NoiDung': row['NoiDung'] if row['NoiDung'] else '',
            'ImageURL': row.get('ImageURL', ''),
            'DichVu': row.get('DichVu', ''),
            'ThongTinLienHe': row.get('ThongTinLienHe', ''),
            'DanhGia': row.get('DanhGia (Google Map)', '')
        }
        
        documents.append(Document(
            page_content=page_content,
            metadata=metadata
        ))
    
    print(f"   Created {len(documents)} documents")
    return documents


# ============================================================================
# VECTOR STORE INITIALIZATION
# ============================================================================

def initialize_embeddings():
    """
    Initialize HuggingFace embeddings model.
    
    Returns:
        HuggingFaceEmbeddings instance
    """
    print("ðŸ¤– Initializing embedding model...")
    print(f"   Model: {EMBEDDING_MODEL}")
    
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'},  # Use 'cuda' if GPU available
        encode_kwargs={'normalize_embeddings': True}  # Normalize for cosine similarity
    )
    
    return embeddings


def create_vector_store(documents: List[Document], embeddings, persist_directory: str) -> Chroma:
    """
    Create new ChromaDB vector store from documents.
    
    Purpose:
    - Convert all documents to vectors
    - Store vectors in ChromaDB for fast similarity search
    - Persist to disk for reuse
    
    Args:
        documents: List of LangChain Documents
        embeddings: Embedding model instance
        persist_directory: Path to store ChromaDB
    
    Returns:
        Initialized Chroma vector store
    """
    print("ðŸ“¦ Creating ChromaDB vector store...")
    print(f"   This may take a few minutes for {len(documents)} documents...")
    
    # Create vector store (embeds all documents)
    vector_store = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name="vietnam_tourism"
    )
    
    print(f"   âœ… Vector store created and persisted to: {persist_directory}")
    return vector_store


def load_vector_store(embeddings, persist_directory: str) -> Chroma:
    """
    Load previously created vector store from disk.
    
    Purpose: Skip re-embedding on subsequent runs (saves time)
    
    Args:
        embeddings: Embedding model instance
        persist_directory: Path where ChromaDB was persisted
    
    Returns:
        Loaded Chroma vector store
    """
    print("ðŸ“‚ Loading existing vector store...")
    
    vector_store = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings,
        collection_name="vietnam_tourism"
    )
    
    print("   âœ… Vector store loaded")
    return vector_store


# ============================================================================
# LLM INITIALIZATION
# ============================================================================

def initialize_llm(api_key: Optional[str] = None, temperature: Optional[float] = None):
    """
    Initialize Google Gemini LLM.
    
    Purpose: Create LLM instance for generating recommendations
    
    Args:
        api_key: Google Gemini API key (optional, will use config if not provided)
        temperature: LLM temperature for creativity (0.0-1.0, optional, will use config if not provided)
    
    Returns:
        ChatGoogleGenerativeAI instance
    """
    print("ðŸ¤– Initializing Google Gemini LLM...")
    
    # Use provided API key or fall back to config
    key_to_use = api_key or GEMINI_API_KEY
    if key_to_use:
        os.environ['GOOGLE_API_KEY'] = key_to_use
    
    # Use provided temperature or fall back to config
    temp_to_use = temperature if temperature is not None else LLM_TEMPERATURE
    
    llm = ChatGoogleGenerativeAI(
        model=GEMINI_MODEL,
        temperature=temp_to_use,
        convert_system_message_to_human=True
    )
    
    print("   âœ… LLM initialized")
    return llm


# ============================================================================
# MAIN RECOMMENDATION FUNCTION
# ============================================================================

def generate_recommendation(
    vector_store: Chroma,
    llm: ChatGoogleGenerativeAI,
    user_query: str,
    user_visited_ids: List[str],
    allow_revisit: bool = False,
    top_k: int = TOP_K_RESULTS,
    verbose: bool = False
) -> Dict:
    """
    Generate personalized tourism recommendations using RAG pipeline.
    
    Pipeline:
    1. Vector Search: Find top_k most relevant locations
    2. History Filtering: Separate new vs visited places
    3. Context Building: Prepare information for LLM
    4. LLM Generation: Create human-friendly recommendations
    
    Args:
        vector_store: ChromaDB vector store instance
        llm: LLM instance for generation
        user_query: Natural language query (e.g., "beautiful waterfalls")
        user_visited_ids: List of loc_ids user has visited
        allow_revisit: If False, exclude visited places from recommendations
        top_k: Number of similar locations to retrieve
        verbose: If True, print detailed pipeline logs
    
    Returns:
        Dictionary containing:
        - final_response: LLM-generated recommendation text
        - new_places: List of unvisited locations
        - old_places: List of visited locations
        - filtered_count: Number of places filtered out
    """
    if verbose:
        print(f"\n{'='*60}")
        print("ðŸ” STARTING RAG RECOMMENDATION PIPELINE")
        print(f"{'='*60}")
        print(f"Query: {user_query}")
        print(f"Visited IDs: {user_visited_ids}")
        print(f"Allow Revisit: {allow_revisit}")
        print(f"{'='*60}\n")
    
    # STEP 1: RAG Retrieval - Vector Similarity Search
    if verbose:
        print("ðŸ“Š STEP 1: Vector Similarity Search")
        print(f"   Searching for top {top_k} similar locations...")
    
    retrieved_docs = vector_store.similarity_search(
        user_query,
        k=top_k
    )
    
    if verbose:
        print(f"   âœ… Retrieved {len(retrieved_docs)} locations\n")
    
    # STEP 2: History Filtering - Separate new vs visited places
    if verbose:
        print("ðŸ”€ STEP 2: History Filtering")
        print(f"   Separating visited vs new places...")
    
    new_places = []  # Places user hasn't visited
    old_places = []  # Places user has visited
    
    for doc in retrieved_docs:
        loc_id = doc.metadata['loc_id']
        
        if loc_id in user_visited_ids:
            old_places.append(doc)
        else:
            new_places.append(doc)
    
    if verbose:
        print(f"   New places: {len(new_places)}")
        print(f"   Visited places: {len(old_places)}\n")
    
    # Decide which places to recommend based on allow_revisit
    if allow_revisit:
        # Include all places (visited + new)
        final_places = retrieved_docs
        filtered_count = 0
        if verbose:
            print("   âœ… Including all places (revisit allowed)\n")
    else:
        # Only recommend new places
        final_places = new_places
        filtered_count = len(old_places)
        if verbose:
            print(f"   âœ… Excluding {filtered_count} visited places\n")
    
    # Handle case where no places remain after filtering
    if not final_places:
        return {
            'final_response': "KhÃ´ng tÃ¬m tháº¥y Ä‘á»‹a Ä‘iá»ƒm má»›i phÃ¹ há»£p. Báº¡n Ä‘Ã£ ghÃ© thÄƒm táº¥t cáº£ cÃ¡c Ä‘á»‹a Ä‘iá»ƒm tÆ°Æ¡ng tá»±. HÃ£y thá»­ tÃ¬m kiáº¿m vá»›i tá»« khÃ³a khÃ¡c hoáº·c cho phÃ©p ghÃ© láº¡i cÃ¡c Ä‘á»‹a Ä‘iá»ƒm Ä‘Ã£ thÄƒm.",
            'new_places': [],
            'old_places': old_places,
            'filtered_count': filtered_count
        }
    
    # STEP 3: Context Building - Prepare data for LLM
    if verbose:
        print("ðŸ“ STEP 3: Building Context for LLM")
    
    # Build structured context from final places
    context_parts = []
    for i, doc in enumerate(final_places, 1):
        meta = doc.metadata
        
        place_info = f"""
Äá»‹a Ä‘iá»ƒm {i}:
- TÃªn: {meta['TenDiaDanh']}
- Äá»‹a chá»‰: {meta['DiaChi']}
"""
        
        # Add description if available
        if meta.get('NoiDung') and meta['NoiDung'].strip():
            place_info += f"- MÃ´ táº£: {meta['NoiDung']}\n"
        
        # Add rating if available
        if meta.get('DanhGia') and str(meta['DanhGia']).strip() and str(meta['DanhGia']) != 'N/A':
            place_info += f"- ÄÃ¡nh giÃ¡: {meta['DanhGia']}\n"
        
        # Mark if visited (when allow_revisit=True)
        if allow_revisit and meta['loc_id'] in user_visited_ids:
            place_info += "- Tráº¡ng thÃ¡i: ÄÃ£ ghÃ© thÄƒm\n"
        
        context_parts.append(place_info)
    
    context = "\n".join(context_parts)
    
    if verbose:
        print(f"   âœ… Context built for {len(final_places)} places\n")
    
    # STEP 4: LLM Generation - Create recommendation text
    if verbose:
        print("ðŸ¤– STEP 4: Generating LLM Response")
    
    # Create prompt template
    prompt_template = PromptTemplate(
        input_variables=["user_query", "context", "filtered_count"],
        template="""
Báº¡n lÃ  má»™t hÆ°á»›ng dáº«n viÃªn du lá»‹ch Viá»‡t Nam chuyÃªn nghiá»‡p vÃ  thÃ¢n thiá»‡n.

NgÆ°á»i dÃ¹ng Ä‘ang tÃ¬m kiáº¿m: "{user_query}"

Dá»±a trÃªn thÃ´ng tin cÃ¡c Ä‘á»‹a Ä‘iá»ƒm dÆ°á»›i Ä‘Ã¢y, hÃ£y viáº¿t má»™t Ä‘oáº¡n giá»›i thiá»‡u háº¥p dáº«n vÃ  chi tiáº¿t:

{context}

YÃªu cáº§u:
1. Viáº¿t báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn, thÃ¢n thiá»‡n
2. NÃªu rÃµ Ä‘áº·c Ä‘iá»ƒm ná»•i báº­t cá»§a tá»«ng Ä‘á»‹a Ä‘iá»ƒm
3. Gá»£i Ã½ lÃ½ do nÃªn ghÃ© thÄƒm
4. Sáº¯p xáº¿p theo má»©c Ä‘á»™ phÃ¹ há»£p vá»›i yÃªu cáº§u
{filter_note}

HÃ£y viáº¿t Ä‘oáº¡n giá»›i thiá»‡u:
"""
    )
    
    # Add note about filtered places if applicable
    filter_note = ""
    if filtered_count > 0:
        filter_note = f"\n5. LÆ°u Ã½: ÄÃ£ loáº¡i bá» {filtered_count} Ä‘á»‹a Ä‘iá»ƒm mÃ  ngÆ°á»i dÃ¹ng Ä‘Ã£ ghÃ© thÄƒm"
    
    # Generate prompt
    prompt = prompt_template.format(
        user_query=user_query,
        context=context,
        filtered_count=filtered_count,
        filter_note=filter_note
    )
    
    if verbose:
        print("   Calling Gemini API...")
    
    # Call LLM
    response = llm.invoke(prompt)
    final_response = response.content
    
    if verbose:
        print("   âœ… Response generated\n")
        print(f"{'='*60}")
        print("âœ… PIPELINE COMPLETED")
        print(f"{'='*60}\n")
    
    return {
        'final_response': final_response,
        'new_places': new_places,
        'old_places': old_places,
        'filtered_count': filtered_count
    }


async def generate_recommendation_stream(
    vector_store: Chroma,
    llm: ChatGoogleGenerativeAI,
    user_query: str,
    user_visited_ids: List[str],
    allow_revisit: bool = False,
    top_k: int = TOP_K_RESULTS,
    verbose: bool = False
):
    """
    Generate personalized tourism recommendations with streaming support.
    
    This is an async generator that yields tokens from the LLM response,
    allowing for real-time streaming in Chainlit.
    
    Args:
        Same as generate_recommendation()
    
    Yields:
        Tuples of (token, metadata_dict) where:
        - token: String token from LLM
        - metadata_dict: Contains new_places, old_places, filtered_count
    """
    if verbose:
        print(f"\n{'='*60}")
        print("ðŸ” STARTING RAG RECOMMENDATION PIPELINE (STREAMING)")
        print(f"{'='*60}")
        print(f"Query: {user_query}")
        print(f"Visited IDs: {user_visited_ids}")
        print(f"Allow Revisit: {allow_revisit}")
        print(f"{'='*60}\n")
    
    # STEP 1: RAG Retrieval
    retrieved_docs = vector_store.similarity_search(user_query, k=top_k)
    
    # STEP 2: History Filtering
    new_places = []
    old_places = []
    
    for doc in retrieved_docs:
        loc_id = doc.metadata['loc_id']
        if loc_id in user_visited_ids:
            old_places.append(doc)
        else:
            new_places.append(doc)
    
    # Decide which places to recommend
    if allow_revisit:
        final_places = retrieved_docs
        filtered_count = 0
    else:
        final_places = new_places
        filtered_count = len(old_places)
    
    # Handle case where no places remain
    if not final_places:
        yield ("KhÃ´ng tÃ¬m tháº¥y Ä‘á»‹a Ä‘iá»ƒm má»›i phÃ¹ há»£p. Báº¡n Ä‘Ã£ ghÃ© thÄƒm táº¥t cáº£ cÃ¡c Ä‘á»‹a Ä‘iá»ƒm tÆ°Æ¡ng tá»±. "
               "HÃ£y thá»­ tÃ¬m kiáº¿m vá»›i tá»« khÃ³a khÃ¡c hoáº·c cho phÃ©p ghÃ© láº¡i cÃ¡c Ä‘á»‹a Ä‘iá»ƒm Ä‘Ã£ thÄƒm.",
               {
                   'new_places': [],
                   'old_places': old_places,
                   'filtered_count': filtered_count
               })
        return
    
    # STEP 3: Build Context
    context_parts = []
    for i, doc in enumerate(final_places, 1):
        meta = doc.metadata
        place_info = f"""
Äá»‹a Ä‘iá»ƒm {i}:
- TÃªn: {meta['TenDiaDanh']}
- Äá»‹a chá»‰: {meta['DiaChi']}
"""
        if meta.get('NoiDung') and meta['NoiDung'].strip():
            place_info += f"- MÃ´ táº£: {meta['NoiDung']}\n"
        
        if meta.get('DanhGia') and str(meta['DanhGia']).strip() and str(meta['DanhGia']) != 'N/A':
            place_info += f"- ÄÃ¡nh giÃ¡: {meta['DanhGia']}\n"
        
        if allow_revisit and meta['loc_id'] in user_visited_ids:
            place_info += "- Tráº¡ng thÃ¡i: ÄÃ£ ghÃ© thÄƒm\n"
        
        context_parts.append(place_info)
    
    context = "\n".join(context_parts)
    
    # STEP 4: Generate prompt
    filter_note = ""
    if filtered_count > 0:
        filter_note = f"\n5. LÆ°u Ã½: ÄÃ£ loáº¡i bá» {filtered_count} Ä‘á»‹a Ä‘iá»ƒm mÃ  ngÆ°á»i dÃ¹ng Ä‘Ã£ ghÃ© thÄƒm"
    
    prompt = f"""
Báº¡n lÃ  má»™t hÆ°á»›ng dáº«n viÃªn du lá»‹ch Viá»‡t Nam chuyÃªn nghiá»‡p vÃ  thÃ¢n thiá»‡n.

NgÆ°á»i dÃ¹ng Ä‘ang tÃ¬m kiáº¿m: "{user_query}"

Dá»±a trÃªn thÃ´ng tin cÃ¡c Ä‘á»‹a Ä‘iá»ƒm dÆ°á»›i Ä‘Ã¢y, hÃ£y viáº¿t má»™t Ä‘oáº¡n giá»›i thiá»‡u háº¥p dáº«n vÃ  chi tiáº¿t:

{context}

YÃªu cáº§u:
1. Viáº¿t báº±ng tiáº¿ng Viá»‡t tá»± nhiÃªn, thÃ¢n thiá»‡n
2. NÃªu rÃµ Ä‘áº·c Ä‘iá»ƒm ná»•i báº­t cá»§a tá»«ng Ä‘á»‹a Ä‘iá»ƒm
3. Gá»£i Ã½ lÃ½ do nÃªn ghÃ© thÄƒm
4. Sáº¯p xáº¿p theo má»©c Ä‘á»™ phÃ¹ há»£p vá»›i yÃªu cáº§u
{filter_note}

HÃ£y viáº¿t Ä‘oáº¡n giá»›i thiá»‡u:
"""
    
    # STEP 5: Stream LLM response
    metadata = {
        'new_places': new_places,
        'old_places': old_places,
        'filtered_count': filtered_count
    }
    
    async for chunk in llm.astream(prompt):
        yield (chunk.content, metadata)


# ============================================================================
# INITIALIZATION HELPER
# ============================================================================

def initialize_rag_system(
    csv_path: str = CSV_PATH,
    chroma_db_path: str = CHROMA_DB_PATH,
    force_recreate: bool = False,
    api_key: Optional[str] = None
) -> Tuple[Chroma, ChatGoogleGenerativeAI, HuggingFaceEmbeddings]:
    """
    Initialize complete RAG system (one-stop function).
    
    This function handles:
    1. Loading and processing CSV data
    2. Creating or loading vector store
    3. Initializing LLM
    
    Args:
        csv_path: Path to tourism CSV file
        chroma_db_path: Path to ChromaDB storage
        force_recreate: If True, recreate vector store even if exists
        api_key: Google Gemini API key (optional)
    
    Returns:
        Tuple of (vector_store, llm, embeddings)
    """
    print("\n" + "="*60)
    print("ðŸš€ INITIALIZING RAG TOURISM SYSTEM")
    print("="*60 + "\n")
    
    # Initialize embeddings
    embeddings = initialize_embeddings()
    
    # Check if vector store exists
    vector_store_exists = os.path.exists(chroma_db_path)
    
    if vector_store_exists and not force_recreate:
        # Load existing vector store
        print("âœ… Found existing vector store")
        vector_store = load_vector_store(embeddings, chroma_db_path)
    else:
        # Create new vector store
        if force_recreate:
            print("ðŸ”„ Force recreating vector store...")
        else:
            print("ðŸ“¦ Vector store not found, creating new one...")
        
        # Load and process data
        tourism_df = load_and_process_data(csv_path)
        
        # Create documents
        documents = create_documents(tourism_df)
        
        # Create vector store
        vector_store = create_vector_store(documents, embeddings, chroma_db_path)
    
    # Initialize LLM
    llm = initialize_llm(api_key=api_key)
    
    print("\n" + "="*60)
    print("âœ… RAG SYSTEM READY!")
    print("="*60 + "\n")
    
    return vector_store, llm, embeddings


# ============================================================================
# EXPORT
# ============================================================================

__all__ = [
    'slugify',
    'load_and_process_data',
    'create_documents',
    'initialize_embeddings',
    'create_vector_store',
    'load_vector_store',
    'initialize_llm',
    'generate_recommendation',
    'generate_recommendation_stream',
    'initialize_rag_system',
    'CSV_PATH',
    'CHROMA_DB_PATH',
    'EMBEDDING_MODEL',
    'GEMINI_MODEL',
    'TOP_K_RESULTS'
]
