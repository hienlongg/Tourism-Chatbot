"""
Hugging Face Spaces: Vietnamese Tourism Embedding API

This Gradio app exposes the embedding model as a REST API.
Deploy to Hugging Face Spaces to offload embedding computation from your main backend.

Key Features:
- Lightweight Gradio interface (automatic REST API)
- HuggingFace embedding model (all-MiniLM-L6-v2)
- Low latency semantic search
- Scales independently from main backend

Free tier specs: 16GB RAM, 2 vCPUs
Perfect for running the embedding model efficiently.

Usage:
1. Create a Space on HuggingFace with Gradio SDK
2. Upload this file
3. Access API at: https://yourusername-tourism-embedding.hf.space/run/embed_text
"""

import gradio as gr
import numpy as np
from typing import List
import os
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Lazy load to optimize startup time
_embeddings = None
_model_name = "sentence-transformers/all-MiniLM-L6-v2"


def get_embeddings():
    """Lazy load HuggingFace embeddings model on first use."""
    global _embeddings
    if _embeddings is None:
        logger.info(f"Loading embedding model: {_model_name}")
        from langchain_huggingface import HuggingFaceEmbeddings
        
        _embeddings = HuggingFaceEmbeddings(
            model_name=_model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        logger.info("âœ… Embedding model loaded successfully")
    
    return _embeddings


def embed_text(text: str) -> List[float]:
    """
    Embed a single text using HuggingFace embeddings.
    
    Args:
        text: Input text to embed
    
    Returns:
        List of floats representing the embedding vector
    """
    logger.info(f"Embedding text: {text[:100]}...")
    
    embeddings = get_embeddings()
    result = embeddings.embed_query(text)
    
    logger.info(f"âœ… Embedding generated (dim={len(result)})")
    return result


def embed_documents(texts: str) -> List[List[float]]:
    """
    Embed multiple texts (one per line).
    
    Args:
        texts: Multi-line string where each line is a document to embed
    
    Returns:
        List of embedding vectors (one per document)
    """
    docs = [doc.strip() for doc in texts.split('\n') if doc.strip()]
    
    if not docs:
        return []
    
    logger.info(f"Embedding {len(docs)} documents...")
    
    embeddings = get_embeddings()
    results = embeddings.embed_documents(docs)
    
    logger.info(f"âœ… {len(results)} embeddings generated")
    return results


def similarity_search(query: str, num_results: int = 5) -> str:
    """
    Perform similarity search on sample documents.
    
    This is a demo endpoint. In production, your backend will:
    1. Call embed_text(query) to get the query embedding
    2. Use the vector embedding to search ChromaDB
    
    Args:
        query: Search query text
        num_results: Number of results to return
    
    Returns:
        String describing similarity scores
    """
    logger.info(f"Similarity search: {query}")
    
    embeddings = get_embeddings()
    query_embedding = embeddings.embed_query(query)
    
    # This is just a demo response
    # Your backend will handle the actual ChromaDB search
    return f"Query embedded successfully. Vector dimension: {len(query_embedding)}\nUse this embedding to search your ChromaDB instance."


# ============================================================================
# GRADIO INTERFACE
# ============================================================================

with gr.Blocks(title="Tourism Embedding API") as demo:
    gr.Markdown(
        """
        # ðŸ‡»ðŸ‡³ Vietnamese Tourism Embedding API
        
        Hosted on Hugging Face Spaces for efficient embedding computation.
        
        ## Available Endpoints:
        - **POST** `/run/embed_text` - Embed single text
        - **POST** `/run/embed_documents` - Embed multiple texts
        - **POST** `/run/similarity_search` - Demo similarity search
        
        ## Usage Example (from your backend):
        ```python
        import requests
        
        HF_SPACE_URL = "https://yourusername-tourism-embedding.hf.space"
        
        # Embed a single query
        response = requests.post(
            f"{HF_SPACE_URL}/run/embed_text",
            json={"data": ["beautiful waterfalls"]}
        )
        embedding = response.json()["data"][0]
        ```
        """
    )
    
    # Tab 1: Single Text Embedding
    with gr.Tab("Embed Single Text"):
        text_input = gr.Textbox(
            label="Text to embed",
            placeholder="E.g., 'beautiful mountain resort in Vietnam'",
            lines=3
        )
        embed_btn = gr.Button("Embed", variant="primary")
        output = gr.Textbox(label="Embedding Vector (first 10 dims)", interactive=False)
        
        def show_embedding(text):
            emb = embed_text(text)
            return f"Dimension: {len(emb)}\nFirst 10 values: {emb[:10]}"
        
        embed_btn.click(show_embedding, inputs=text_input, outputs=output)
    
    # Tab 2: Batch Embedding
    with gr.Tab("Embed Documents"):
        docs_input = gr.Textbox(
            label="Documents (one per line)",
            placeholder="Doc 1\nDoc 2\nDoc 3",
            lines=5
        )
        batch_btn = gr.Button("Embed All", variant="primary")
        batch_output = gr.Textbox(label="Results", interactive=False)
        
        def show_batch_embeddings(texts):
            results = embed_documents(texts)
            return f"Generated {len(results)} embeddings\nEach with dimension 384"
        
        batch_btn.click(show_batch_embeddings, inputs=docs_input, outputs=batch_output)
    
    # Tab 3: Similarity Search Demo
    with gr.Tab("Similarity Search Demo"):
        query_input = gr.Textbox(
            label="Search query",
            placeholder="E.g., 'beaches near Hanoi'",
        )
        sim_btn = gr.Button("Search", variant="primary")
        sim_output = gr.Textbox(label="Results", interactive=False)
        
        sim_btn.click(similarity_search, inputs=query_input, outputs=sim_output)
    
    gr.Markdown(
        """
        ## How to use in production:
        
        1. **Deploy this Space** on Hugging Face (choose Gradio SDK)
        2. **Get your Space URL** (e.g., `https://yourusername-tourism-embedding.hf.space`)
        3. **Update your backend** to use the remote embedding API:
        
        ```python
        # In your rag_engine.py client module
        class EmbeddingAPIClient:
            def __init__(self, space_url):
                self.space_url = space_url
            
            def embed_query(self, text: str) -> List[float]:
                response = requests.post(
                    f"{self.space_url}/run/embed_text",
                    json={"data": [text]}
                )
                return response.json()["data"][0]
        ```
        
        4. **Keep ChromaDB locally** - only embeddings are offloaded
        5. **Call generate_recommendation()** as before in your backend
        
        ## Benefits:
        - âœ… Free tier: 16GB RAM + 2 vCPUs
        - âœ… Embedding model runs independently
        - âœ… Your main backend stays lightweight
        - âœ… Scales independently from chat logic
        - âœ… Easy zero-downtime updates
        """
    )


if __name__ == "__main__":
    # Launch Gradio app
    # On HF Spaces, this will automatically expose REST API endpoints
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,  # HF Spaces handles sharing
    )
